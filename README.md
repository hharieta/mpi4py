---
theme: gaia
_class: lead
paginate: true
backgroundColor: #fff
backgroundImage: url('https://marp.app/assets/hero-background.svg')
marp: true

---

# Aproximaci√≥n del n√∫mero PI con el m√©todo de Montecarlo ‚õ∞Ô∏è

---

![bg left blur:1px](assets/circulo_inscrito.jpeg)

# Integrantes

* Hern√°ndez Arieta H√©ctor
* Heredia Maas Yesica Sarahi
* Hsiao Tung Ya-Yun
* Le√≥n Puc Jennifer
* Lopez Ciau Carlos David

---

## ‚öúÔ∏è Teor√≠a ‚öúÔ∏è

1. Considera un cuadrado de lado 2u, centrado en el origen (0,0). Este cuadrado tiene un √°rea de 4u cuadradas.
2. Dentro de este cuadrado se haya inscrito un c√≠rculo de radio 1u, tambi√©n centrado en el origen. El √°rea de este c√≠rculo es œÄ unidades cuadradas.
3. Genera puntos aleatorios uniformemente distribuidos dentro del cuadrado.
4. Cuenta cu√°ntos de estos puntos est√°n dentro del c√≠rculo inscrito.
---
5. La raz√≥n entre el n√∫mero de puntos dentro del c√≠rculo y el total de puntos generados se aproxima a la raz√≥n entre el √°rea del c√≠rculo y el √°rea del cuadrado. 
$$\begin{aligned} Raz√≥n = \frac{puntos dentro del c√≠rculo}{puntos totales} \end{aligned}$$
6. Multiplicando esta raz√≥n por 4, obtenemos una estimaci√≥n de œÄ. 
$$\begin{equation} Raz√≥n \times 4 \approx PI \end{equation}$$

---
## ‚öúÔ∏è Interfaz de paso de mensajes (MPI) ‚öúÔ∏è

**MPI** es un est√°ndar de comunicaci√≥n utilizado en programaci√≥n paralela para permitir que procesos en un cl√∫ster se comuniquen entre s√≠ y coordinen su trabajo. MPI se utiliza com√∫nmente en aplicaciones de alto rendimiento y c√≥mputo distribuido.


En Python üêç la biblioteca __mpi4py__ proporciona una interfaz para utilizar MPI en scripts Python

---

### üß© Caracter√≠sticas 

* **Comunicadores**: En MPI, los procesos se organizan en grupos llamados "comunicadores". Cada comunicador tiene un conjunto espec√≠fico de procesos.
* **Env√≠o/Recepci√≥n de Mensajes**: MPI permite que los procesos se comuniquen enviando y recibiendo mensajes entre s√≠. Los mensajes pueden contener datos, como matrices o valores escalares.
* **Operaciones Colectivas**: MPI admite operaciones colectivas que involucran a todos los procesos en un comunicador. Estas operaciones incluyen difusi√≥n `(Bcast)`, reducci√≥n `(Reduce)`, dispersi√≥n `(Scatter)`, recopilaci√≥n `(Gather)`, entre otras.

---

* **Inicio/Fin MPI**: Se debe inicializar el entorno MPI y se debe finalizar al final del programa.
* **Proceso Ra√≠z**: En operaciones colectivas como la reducci√≥n, hay un proceso que act√∫a como el "proceso ra√≠z" que coordina la operaci√≥n. En `mpi4py`, se puede especificar el proceso ra√≠z mediante el par√°metro root en las funciones colectivas.
* **Ejecuci√≥n Paralela**: Cuando se ejecuta un programa MPI en un cl√∫ster o en una m√°quina con m√∫ltiples n√∫cleos, cada proceso MPI se ejecuta de manera independiente en su propio espacio de memoria, y la comunicaci√≥n entre procesos se realiza a trav√©s de MPI.

---

üë®‚Äçüè´ **Ejemplo de uso:**

```python
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

data = 5
result = comm.reduce(data, op=MPI.SUM, root=0)

if rank == 0: print(f"La suma total es: {result}")
``````

para ejecutar el script 

```bash
mpirun --oversubscribe -n 6 python suma.py
```

---

 **C√≥digo: Script Principal** üë®‚Äçüíª‚Äã

```python
#!/usr/bin/env python
from mpi4py import MPI
import numpy
import sys

#  inicia un nuevo conjunto de procesos MPI,
#  ejecutando el script cpi.py en cada uno de ellos.
#  maxprocs indica n procesos MPI para ejecutar el script.
comm = MPI.COMM_SELF.Spawn(sys.executable,
                           args=['child.py'],
                           maxprocs=6)
N = numpy.array(10, 'int64')
# env√≠a el valor de N a todos los procesos MPI
# desde el proceso ra√≠z (MPI.ROOT) 
# asegura que todos los procesos tengan 
# el mismo valor de N para su c√°lculo.
comm.Bcast([N, MPI.INT], root=MPI.ROOT)

PI = numpy.array(0.0, 'float64')

# reduce los valores de PI calculados por cada proceso
# en un solo valor, sumando los valores de PI
comm.Reduce(None, [PI, MPI.DOUBLE],
            op=MPI.SUM, root=MPI.ROOT)
print(PI)

# desconecta el comunicador
comm.Disconnect()
```

---

**C√≥digo: Script Secundario** üêç

```python
#!/usr/bin/env python
from mpi4py import MPI
import numpy

# Obtiene informaci√≥n sobre el comunicador padre
comm = MPI.Comm.Get_parent()

# Obtiene el tama√±o del proceso
size = comm.Get_size()
# Obtiene el rango del proceso
rank = comm.Get_rank()

N = numpy.array(0, dtype='int64')

# Recibe el valor de N transmitido por el script padre
comm.Bcast([N, MPI.INT], root=0)

# Calcula el valor de PI
#  con el m√©todo de Monte Carlo.
h = 1.0 / N; s = 0.0
for i in range(rank, N, size):
    # print(rank, N, size, i)
    x = h * (i + 0.5)
    s += 4.0 / (1.0 + x**2)
    # print(x, h, s)

# Reduce los valores de PI calculados por cada proceso
PI = numpy.array(s * h, dtype='float64')
comm.Reduce([PI, MPI.DOUBLE], None,
            op=MPI.SUM, root=0)

# Desconecta el comunicador
comm.Disconnect()
```

---

![bg right](assets/tanque.jpg)

# Tank U

Made by Gatovsky üò∫


_Todos los derechos reservados_


